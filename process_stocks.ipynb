{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e8193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca38cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: SBIN.csv with 284 rows\n",
      "‚úÖ Saved: BAJFINANCE.csv with 284 rows\n",
      "‚úÖ Saved: TITAN.csv with 284 rows\n",
      "‚úÖ Saved: ITC.csv with 284 rows\n",
      "‚úÖ Saved: TCS.csv with 284 rows\n",
      "‚úÖ Saved: LT.csv with 284 rows\n",
      "‚úÖ Saved: TATACONSUM.csv with 284 rows\n",
      "‚úÖ Saved: RELIANCE.csv with 284 rows\n",
      "‚úÖ Saved: HCLTECH.csv with 284 rows\n",
      "‚úÖ Saved: JSWSTEEL.csv with 284 rows\n",
      "‚úÖ Saved: ULTRACEMCO.csv with 284 rows\n",
      "‚úÖ Saved: POWERGRID.csv with 284 rows\n",
      "‚úÖ Saved: INFY.csv with 284 rows\n",
      "‚úÖ Saved: TRENT.csv with 284 rows\n",
      "‚úÖ Saved: BHARTIARTL.csv with 284 rows\n",
      "‚úÖ Saved: TATAMOTORS.csv with 284 rows\n",
      "‚úÖ Saved: WIPRO.csv with 284 rows\n",
      "‚úÖ Saved: TECHM.csv with 284 rows\n",
      "‚úÖ Saved: NTPC.csv with 284 rows\n",
      "‚úÖ Saved: HINDUNILVR.csv with 284 rows\n",
      "‚úÖ Saved: APOLLOHOSP.csv with 284 rows\n",
      "‚úÖ Saved: M&M.csv with 284 rows\n",
      "‚úÖ Saved: GRASIM.csv with 284 rows\n",
      "‚úÖ Saved: ICICIBANK.csv with 284 rows\n",
      "‚úÖ Saved: ADANIENT.csv with 284 rows\n",
      "‚úÖ Saved: ADANIPORTS.csv with 284 rows\n",
      "‚úÖ Saved: BEL.csv with 284 rows\n",
      "‚úÖ Saved: BAJAJFINSV.csv with 284 rows\n",
      "‚úÖ Saved: EICHERMOT.csv with 284 rows\n",
      "‚úÖ Saved: COALINDIA.csv with 284 rows\n",
      "‚úÖ Saved: MARUTI.csv with 284 rows\n",
      "‚úÖ Saved: INDUSINDBK.csv with 284 rows\n",
      "‚úÖ Saved: ASIANPAINT.csv with 284 rows\n",
      "‚úÖ Saved: TATASTEEL.csv with 284 rows\n",
      "‚úÖ Saved: HDFCLIFE.csv with 284 rows\n",
      "‚úÖ Saved: DRREDDY.csv with 284 rows\n",
      "‚úÖ Saved: SUNPHARMA.csv with 284 rows\n",
      "‚úÖ Saved: KOTAKBANK.csv with 284 rows\n",
      "‚úÖ Saved: SHRIRAMFIN.csv with 284 rows\n",
      "‚úÖ Saved: NESTLEIND.csv with 284 rows\n",
      "‚úÖ Saved: ONGC.csv with 284 rows\n",
      "‚úÖ Saved: CIPLA.csv with 284 rows\n",
      "‚úÖ Saved: BPCL.csv with 284 rows\n",
      "‚úÖ Saved: BRITANNIA.csv with 284 rows\n",
      "‚úÖ Saved: SBILIFE.csv with 284 rows\n",
      "‚úÖ Saved: HINDALCO.csv with 284 rows\n",
      "‚úÖ Saved: HEROMOTOCO.csv with 284 rows\n",
      "‚úÖ Saved: AXISBANK.csv with 284 rows\n",
      "‚úÖ Saved: HDFCBANK.csv with 284 rows\n",
      "‚úÖ Saved: BAJAJ-AUTO.csv with 284 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# ‚úÖ Change this to your *main parent* folder path that contains 14 subfolders\n",
    "main_folder_path = r\"D:\\2 project dataset\" # üîÅ CHANGE this to your correct folder\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# ‚úÖ Step 1: Walk through all folders and collect YAML data\n",
    "for root, dirs, files in os.walk(main_folder_path):\n",
    "    for filename in files:\n",
    "        if filename.endswith(\".yaml\") or filename.endswith(\".yml\"):\n",
    "            file_path = os.path.join(root, filename)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    data = yaml.safe_load(f)\n",
    "                    if data:\n",
    "                        df = pd.DataFrame(data)\n",
    "                        df_list.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error reading {file_path}: {e}\")\n",
    "\n",
    "# ‚úÖ Step 2: Concatenate all DataFrames\n",
    "if not df_list:\n",
    "    print(\"‚ùå No data found in YAML files!\")\n",
    "    exit()\n",
    "\n",
    "full_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# ‚úÖ Step 3: Handle date column with try/except\n",
    "try:\n",
    "    full_df['date'] = pd.to_datetime(full_df['date'], errors='coerce')\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Date parsing failed: {e}\")\n",
    "\n",
    "# ‚úÖ Step 4: Drop rows where date couldn't be parsed\n",
    "full_df = full_df.dropna(subset=['date'])\n",
    "\n",
    "# ‚úÖ Step 5: Save separate CSV for each ticker\n",
    "output_folder = \"ticker_csv_output\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "for ticker in full_df['Ticker'].unique():\n",
    "    try:\n",
    "        ticker_df = full_df[full_df['Ticker'] == ticker].sort_values('date')\n",
    "        ticker_df.to_csv(os.path.join(output_folder, f\"{ticker}.csv\"), index=False)\n",
    "        print(f\"‚úÖ Saved: {ticker}.csv with {len(ticker_df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save CSV for {ticker}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a73a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker   close                 date    high     low    month   open  \\\n",
      "0   SBIN  602.95  2023-10-03 05:30:00  604.90  589.60  2023-10  596.6   \n",
      "1   SBIN  586.25  2023-10-04 05:30:00  600.45  584.45  2023-10  600.0   \n",
      "2   SBIN  592.15  2023-10-05 05:30:00  594.35  587.10  2023-10  590.0   \n",
      "3   SBIN  594.25  2023-10-06 05:30:00  598.95  592.20  2023-10  593.4   \n",
      "4   SBIN  585.10  2023-10-09 05:30:00  589.00  581.55  2023-10  588.0   \n",
      "\n",
      "     volume  \n",
      "0  15322196  \n",
      "1  24914612  \n",
      "2  13248028  \n",
      "3   8216780  \n",
      "4   9189597  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"ticker_csv_output/SBIN.csv\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bab7a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                COMPANY           sector                          Symbol\n",
      "0     ADANI ENTERPRISES    MISCELLANEOUS   ADANI ENTERPRISES: ADANIGREEN\n",
      "1     ADANI PORTS & SEZ    MISCELLANEOUS   ADANI PORTS & SEZ: ADANIPORTS\n",
      "2      APOLLO HOSPITALS    MISCELLANEOUS    APOLLO HOSPITALS: APOLLOHOSP\n",
      "3          ASIAN PAINTS           PAINTS        ASIAN PAINTS: ASIANPAINT\n",
      "4             AXIS BANK          BANKING             AXIS BANK: AXISBANK\n",
      "5            BAJAJ AUTO      AUTOMOBILES          BAJAJ AUTO: BAJAJ-AUTO\n",
      "6         BAJAJ FINANCE          FINANCE       BAJAJ FINANCE: BAJFINANCE\n",
      "7         BAJAJ FINSERV          FINANCE       BAJAJ FINSERV: BAJAJFINSV\n",
      "8    BHARAT ELECTRONICS          DEFENCE         BHARAT ELECTRONICS: BEL\n",
      "9         BHARTI AIRTEL          TELECOM           BHARTI AIRTEL: AIRTEL\n",
      "10                 BPCL           ENERGY                      BPCL: BPCL\n",
      "11                CIPLA  PHARMACEUTICALS                    CIPLA: CIPLA\n",
      "12           COAL INDIA           MINING           COAL INDIA: COALINDIA\n",
      "13       DR. REDDYS LAB  PHARMACEUTICALS         DR. REDDYS LAB: DRREDDY\n",
      "14        EICHER MOTORS      AUTOMOBILES        EICHER MOTORS: EICHERMOT\n",
      "15               GRASIM         TEXTILES                  GRASIM: GRASIM\n",
      "16     HCL TECHNOLOGIES         SOFTWARE       HCL TECHNOLOGIES: HCLTECH\n",
      "17            HDFC BANK          BANKING             HDFC BANK: HDFCBANK\n",
      "18  HDFC LIFE INSURANCE        INSURANCE   HDFC LIFE INSURANCE: HDFCLIFE\n",
      "19        HERO MOTOCORP      AUTOMOBILES       HERO MOTOCORP: HEROMOTOCO\n",
      "20             HINDALCO        ALUMINIUM              HINDALCO: HINDALCO\n",
      "21   HINDUSTAN UNILEVER             FMCG  HINDUSTAN UNILEVER: HINDUNILVR\n",
      "22           ICICI BANK          BANKING           ICICI BANK: ICICIBANK\n",
      "23        INDUSIND BANK          BANKING       INDUSIND BANK: INDUSINDBK\n",
      "24              INFOSYS         SOFTWARE                   INFOSYS: INFY\n",
      "25                  IOC           ENERGY                        IOC: IOC\n",
      "26                  ITC   FOOD & TOBACCO                        ITC: ITC\n",
      "27            JSW STEEL            STEEL             JSW STEEL: JSWSTEEL\n",
      "28  KOTAK MAHINDRA BANK          BANKING  KOTAK MAHINDRA BANK: KOTAKBANK\n",
      "29                  L&T      ENGINEERING                         L&T: LT\n",
      "30                  M&M      AUTOMOBILES                        M&M: M&M\n",
      "31        MARUTI SUZUKI      AUTOMOBILES           MARUTI SUZUKI: MARUTI\n",
      "32               NESTLE   FOOD & TOBACCO               NESTLE: NESTLEIND\n",
      "33                 NTPC            POWER                      NTPC: NTPC\n",
      "34                 ONGC           ENERGY                      ONGC: ONGC\n",
      "35           POWER GRID            POWER           POWER GRID: POWERGRID\n",
      "36        RELIANCE IND.           ENERGY         RELIANCE IND.: RELIANCE\n",
      "37                  SBI          BANKING                       SBI: SBIN\n",
      "38   SBI LIFE INSURANCE        INSURANCE     SBI LIFE INSURANCE: SBILIFE\n",
      "39      SHRIRAM FINANCE          FINANCE     SHRIRAM FINANCE: SHRIRAMFIN\n",
      "40           SUN PHARMA  PHARMACEUTICALS           SUN PHARMA: SUNPHARMA\n",
      "41        TATA CONSUMER             FMCG     TATA CONSUMER: TATACONSUMER\n",
      "42          TATA MOTORS      AUTOMOBILES         TATA MOTORS: TATAMOTORS\n",
      "43           TATA STEEL            STEEL           TATA STEEL: TATASTEEL\n",
      "44                  TCS         SOFTWARE                        TCS: TCS\n",
      "45        TECH MAHINDRA         SOFTWARE            TECH MAHINDRA: TECHM\n",
      "46                TITAN        RETAILING                    TITAN: TITAN\n",
      "47                TRENT        RETAILING                    TRENT: TRENT\n",
      "48     ULTRATECH CEMENT           CEMENT    ULTRATECH CEMENT: ULTRACEMCO\n",
      "49                WIPRO         SOFTWARE                    WIPRO: WIPRO\n"
     ]
    }
   ],
   "source": [
    "sector_df = pd.read_csv(\"C:/Users/Office/Downloads/Sector_data - Sheet1.csv\")\n",
    "print(sector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925b6c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de6e1ece",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sector_mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSector mapping contains tickers:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(sector_mapping\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m50\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sector_mapping' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Sector mapping contains tickers:\", list(sector_mapping.keys())[:50])  # preview first 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e10c4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sector added: ADANIENT.csv\n",
      "‚úÖ Sector added: ADANIPORTS.csv\n",
      "‚úÖ Sector added: APOLLOHOSP.csv\n",
      "‚úÖ Sector added: ASIANPAINT.csv\n",
      "‚úÖ Sector added: AXISBANK.csv\n",
      "‚úÖ Sector added: BAJAJ-AUTO.csv\n",
      "‚úÖ Sector added: BAJAJFINSV.csv\n",
      "‚úÖ Sector added: BAJFINANCE.csv\n",
      "‚úÖ Sector added: BEL.csv\n",
      "‚úÖ Sector added: BHARTIARTL.csv\n",
      "‚úÖ Sector added: BPCL.csv\n",
      "‚úÖ Sector added: BRITANNIA.csv\n",
      "‚úÖ Sector added: CIPLA.csv\n",
      "‚úÖ Sector added: COALINDIA.csv\n",
      "‚úÖ Sector added: DRREDDY.csv\n",
      "‚úÖ Sector added: EICHERMOT.csv\n",
      "‚úÖ Sector added: GRASIM.csv\n",
      "‚úÖ Sector added: HCLTECH.csv\n",
      "‚úÖ Sector added: HDFCBANK.csv\n",
      "‚úÖ Sector added: HDFCLIFE.csv\n",
      "‚úÖ Sector added: HEROMOTOCO.csv\n",
      "‚úÖ Sector added: HINDALCO.csv\n",
      "‚úÖ Sector added: HINDUNILVR.csv\n",
      "‚úÖ Sector added: ICICIBANK.csv\n",
      "‚úÖ Sector added: INDUSINDBK.csv\n",
      "‚úÖ Sector added: INFY.csv\n",
      "‚úÖ Sector added: ITC.csv\n",
      "‚úÖ Sector added: JSWSTEEL.csv\n",
      "‚úÖ Sector added: KOTAKBANK.csv\n",
      "‚úÖ Sector added: LT.csv\n",
      "‚úÖ Sector added: M&M.csv\n",
      "‚úÖ Sector added: MARUTI.csv\n",
      "‚úÖ Sector added: NESTLEIND.csv\n",
      "‚úÖ Sector added: NTPC.csv\n",
      "‚úÖ Sector added: ONGC.csv\n",
      "‚úÖ Sector added: POWERGRID.csv\n",
      "‚úÖ Sector added: RELIANCE.csv\n",
      "‚úÖ Sector added: SBILIFE.csv\n",
      "‚úÖ Sector added: SBIN.csv\n",
      "‚úÖ Sector added: SHRIRAMFIN.csv\n",
      "‚úÖ Sector added: SUNPHARMA.csv\n",
      "‚ùå Sector not mapped in TATACONSUM.csv for tickers: ['TATACONSUM']\n",
      "‚úÖ Sector added: TATAMOTORS.csv\n",
      "‚úÖ Sector added: TATASTEEL.csv\n",
      "‚úÖ Sector added: TCS.csv\n",
      "‚úÖ Sector added: TECHM.csv\n",
      "‚úÖ Sector added: TITAN.csv\n",
      "‚úÖ Sector added: TRENT.csv\n",
      "‚úÖ Sector added: ULTRACEMCO.csv\n",
      "‚úÖ Sector added: WIPRO.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# üü© Load the sector mapping file\n",
    "sector_csv = r\"C:\\Users\\Office\\Downloads\\Sector_data - Sheet1.csv\"\n",
    "sector_df = pd.read_csv(sector_csv)\n",
    "\n",
    "# üéØ Extract ticker properly\n",
    "sector_df['ExtractedTicker'] = sector_df['Symbol'].str.split(':').str[-1].str.strip().str.upper()\n",
    "sector_mapping = dict(zip(sector_df['ExtractedTicker'], sector_df['sector']))\n",
    "\n",
    "# ‚úÖ Manually patch missing tickers\n",
    "sector_mapping[\"ADANIENT\"] = \"MISCELLANEOUS\"\n",
    "sector_mapping[\"BHARTIARTL\"] = \"TELECOM\"\n",
    "sector_mapping[\"BRITANNIA\"]  = \"BRITTANIA INDUSTRIESLTD\"\n",
    "\n",
    "# üìÅ Folder containing your 50 CSV files\n",
    "input_folder = r\"C:\\Users\\Office\\Desktop\\project 2\\ticker_csv_output\"\n",
    "\n",
    "# üîÅ Loop through files and apply sector mapping\n",
    "for file in os.listdir(input_folder):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join(input_folder, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if 'Ticker' in df.columns:\n",
    "            df['Ticker'] = df['Ticker'].str.strip().str.upper()\n",
    "            \n",
    "            # ‚úÖ Apply sector mapping *after it's defined*\n",
    "            df['sector'] = df['Ticker'].map(sector_mapping)\n",
    "\n",
    "            # üß™ Check for mapping issues\n",
    "            if df['sector'].isnull().any():\n",
    "                print(f\"‚ùå Sector not mapped in {file} for tickers: {df[df['sector'].isnull()]['Ticker'].unique()}\")\n",
    "            else:\n",
    "                print(f\"‚úÖ Sector added: {file}\")\n",
    "\n",
    "            # üíæ Save updated file\n",
    "            df.to_csv(file_path, index=False)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Skipped {file}: No 'Ticker' column found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a757b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Ticker  Volatility\n",
      "0    ADANIENT    0.028601\n",
      "1  ADANIPORTS    0.026029\n",
      "2  APOLLOHOSP    0.014135\n",
      "3  ASIANPAINT    0.012665\n",
      "4    AXISBANK    0.015625\n",
      "     Ticker                date    close  Daily_Return  cumulative_return\n",
      "0  ADANIENT 2023-10-03 05:30:00  2387.25           NaN           0.000000\n",
      "1  ADANIENT 2023-10-04 05:30:00  2464.95      0.032548           0.032548\n",
      "2  ADANIENT 2023-10-05 05:30:00  2466.35      0.000568           0.033134\n",
      "3  ADANIENT 2023-10-06 05:30:00  2478.10      0.004764           0.038056\n",
      "4  ADANIENT 2023-10-09 05:30:00  2442.60     -0.014325           0.023186\n",
      "        sector  Year  Yearly_Return\n",
      "0    ALUMINIUM  2023       0.281071\n",
      "1    ALUMINIUM  2024       0.068316\n",
      "2  AUTOMOBILES  2023       0.223617\n",
      "3  AUTOMOBILES  2024       0.276492\n",
      "4      BANKING  2023       0.089291\n",
      "Ticker      ADANIENT  ADANIPORTS  APOLLOHOSP  ASIANPAINT  AXISBANK  \\\n",
      "Ticker                                                               \n",
      "ADANIENT    1.000000    0.874233    0.136501    0.291583  0.297736   \n",
      "ADANIPORTS  0.874233    1.000000    0.168902    0.274045  0.380514   \n",
      "APOLLOHOSP  0.136501    0.168902    1.000000    0.258055  0.179381   \n",
      "ASIANPAINT  0.291583    0.274045    0.258055    1.000000  0.126055   \n",
      "AXISBANK    0.297736    0.380514    0.179381    0.126055  1.000000   \n",
      "\n",
      "Ticker      BAJAJ-AUTO  BAJAJFINSV  BAJFINANCE       BEL  BHARTIARTL  ...  \\\n",
      "Ticker                                                                ...   \n",
      "ADANIENT      0.201871    0.356547    0.336031  0.522680    0.282859  ...   \n",
      "ADANIPORTS    0.204456    0.398041    0.398769  0.585672    0.316537  ...   \n",
      "APOLLOHOSP    0.223900    0.249392    0.223142  0.224594    0.236810  ...   \n",
      "ASIANPAINT    0.167650    0.248090    0.296922  0.156953    0.146953  ...   \n",
      "AXISBANK      0.198477    0.393683    0.354079  0.330512    0.238602  ...   \n",
      "\n",
      "Ticker      SUNPHARMA  TATACONSUM  TATAMOTORS  TATASTEEL       TCS     TECHM  \\\n",
      "Ticker                                                                         \n",
      "ADANIENT     0.166256    0.203847    0.324071   0.423563  0.115790  0.141677   \n",
      "ADANIPORTS   0.215856    0.220655    0.339241   0.466043  0.132423  0.148034   \n",
      "APOLLOHOSP   0.286227    0.096700    0.212357   0.254731  0.106846  0.169118   \n",
      "ASIANPAINT   0.202262    0.244278    0.105847   0.287050  0.166699  0.162959   \n",
      "AXISBANK     0.143991    0.139747    0.232145   0.370957  0.094669  0.136469   \n",
      "\n",
      "Ticker         TITAN     TRENT  ULTRACEMCO     WIPRO  \n",
      "Ticker                                                \n",
      "ADANIENT    0.283281  0.190412    0.361152  0.224003  \n",
      "ADANIPORTS  0.296148  0.201076    0.402489  0.238986  \n",
      "APOLLOHOSP  0.253361  0.089516    0.272951  0.223738  \n",
      "ASIANPAINT  0.307404  0.223094    0.280647  0.140642  \n",
      "AXISBANK    0.145931  0.199748    0.397024  0.207802  \n",
      "\n",
      "[5 rows x 50 columns]\n",
      "       Month      Ticker     return        Type\n",
      "0    2023-10   NESTLEIND   8.602208  Top Gainer\n",
      "1    2023-10   COALINDIA   7.656732  Top Gainer\n",
      "2    2023-10  BAJAJ-AUTO   5.932482  Top Gainer\n",
      "3    2023-10     SBILIFE   5.833881  Top Gainer\n",
      "4    2023-10       TRENT   4.642805  Top Gainer\n",
      "..       ...         ...        ...         ...\n",
      "135  2024-11    ADANIENT -24.461773   Top Loser\n",
      "136  2024-11  ADANIPORTS -18.477481   Top Loser\n",
      "137  2024-11  ASIANPAINT -15.907274   Top Loser\n",
      "138  2024-11   BRITANNIA -14.837389   Top Loser\n",
      "139  2024-11        NTPC -11.158381   Top Loser\n",
      "\n",
      "[140 rows x 4 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Office\\AppData\\Local\\Temp\\ipykernel_16156\\3136427059.py:115: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top5 = monthly.groupby('Month').apply(lambda x: x.sort_values('return', ascending=False).head(5))\n",
      "C:\\Users\\Office\\AppData\\Local\\Temp\\ipykernel_16156\\3136427059.py:118: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  bottom5 = monthly.groupby('Month').apply(lambda x: x.sort_values('return').head(5))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ‚úÖ Update this path to your folder containing all 50 stock CSVs\n",
    "folder_path = r\"C:\\Users\\Office\\Desktop\\project 2\\ticker_csv_output\"\n",
    "\n",
    "# üîÅ Combine all files into one DataFrame\n",
    "all_data = pd.DataFrame()\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "# üîÉ Sort to ensure correct return calculation\n",
    "all_data = all_data.sort_values(by=['Ticker', 'date'])\n",
    "\n",
    "#1\n",
    "\n",
    "# üóì Ensure date is datetime and data is sorted\n",
    "all_data['date'] = pd.to_datetime(all_data['date'])\n",
    "\n",
    "# üîô Calculate previous close\n",
    "all_data['Previous_Close'] = all_data.groupby('Ticker')['close'].shift(1)\n",
    "\n",
    "# üìà Calculate Daily Return\n",
    "all_data['Daily_Return'] = (all_data['close'] - all_data['Previous_Close']) / all_data['Previous_Close']\n",
    "\n",
    "# üìä Calculate Volatility per stock (standard deviation of daily return)\n",
    "volatility_df = all_data.groupby('Ticker')['Daily_Return'].std().reset_index()\n",
    "\n",
    "# üè∑ Rename column\n",
    "volatility_df.columns = ['Ticker', 'Volatility']\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ Save to CSV (optional)\n",
    "volatility_df.to_csv(\"volatility_analysis.csv\", index=False)\n",
    "\n",
    "# üîç Preview\n",
    "print(volatility_df.head())\n",
    "\n",
    "\n",
    "\n",
    "#2\n",
    "# Calculate cumulative returns\n",
    "all_data['cumulative_return'] = (1 + all_data['Daily_Return'].fillna(0)).groupby(all_data['Ticker']).cumprod() - 1\n",
    "\n",
    "# Select relevant columns for the final DataFrame\n",
    "cumulative_df = all_data[['Ticker', 'date', 'close', 'Daily_Return', 'cumulative_return']]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "cumulative_df.to_csv(\"cumulative_return_analysis.csv\", index=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(cumulative_df.head())\n",
    "\n",
    "\n",
    "#3\n",
    "\n",
    "#  Sort and prepare data\n",
    "all_data = all_data.sort_values(by=['Ticker', 'date'])\n",
    "all_data['Year'] = all_data['date'].dt.year\n",
    "\n",
    "#  Calculate start and end price per year per stock\n",
    "yearly = all_data.groupby(['Ticker', 'sector', 'Year']).agg(\n",
    "    Start_Price=('close', 'first'),\n",
    "    End_Price=('close', 'last')\n",
    ").reset_index()\n",
    "\n",
    "#  Calculate yearly return\n",
    "yearly['Yearly_Return'] = (yearly['End_Price'] - yearly['Start_Price']) / yearly['Start_Price']\n",
    "\n",
    "#  Group by sector to get average yearly return\n",
    "sector_performance = yearly.groupby(['sector', 'Year'])['Yearly_Return'].mean().reset_index()\n",
    "\n",
    "#  Save or use the DataFrame\n",
    "sector_performance.to_csv(\"sector_wise_performance.csv\", index=False)\n",
    "print(sector_performance.head())\n",
    "\n",
    "\n",
    "#4\n",
    "\n",
    "# Pivot the data to get closing prices in wide format (each column is a stock)\n",
    "pivot_df = all_data.pivot(index='date', columns='Ticker', values='close')\n",
    "\n",
    "# Calculate daily returns (percentage change)\n",
    "returns_df = pivot_df.pct_change()\n",
    "\n",
    "# Compute the correlation matrix of daily returns\n",
    "correlation_matrix = returns_df.corr()\n",
    "\n",
    "# Save as CSV (optional)\n",
    "correlation_matrix.to_csv(\"stock_price_correlation_matrix.csv\")\n",
    "\n",
    "# Preview\n",
    "print(correlation_matrix.head())\n",
    "\n",
    "\n",
    "#5\n",
    "\n",
    "# Convert 'date' to datetime format\n",
    "all_data['date'] = pd.to_datetime(all_data['date'])\n",
    "\n",
    "# Create 'Month' column like '2024-01', '2024-02'\n",
    "all_data['Month'] = all_data['date'].dt.to_period('M')\n",
    "\n",
    "# Group by 'Ticker' and 'Month', and get first and last closing price\n",
    "monthly = all_data.groupby(['Ticker', 'Month'])['close'].agg(['first', 'last']).reset_index()\n",
    "\n",
    "# Calculate monthly return in percentage\n",
    "monthly['return'] = (monthly['last'] - monthly['first']) / monthly['first'] * 100\n",
    "\n",
    "# Get top 5 gainers for each month\n",
    "top5 = monthly.groupby('Month').apply(lambda x: x.sort_values('return', ascending=False).head(5))\n",
    "\n",
    "# Get top 5 losers for each month\n",
    "bottom5 = monthly.groupby('Month').apply(lambda x: x.sort_values('return').head(5))\n",
    "\n",
    "# Add a column to indicate type\n",
    "top5['Type'] = 'Top Gainer'\n",
    "bottom5['Type'] = 'Top Loser'\n",
    "\n",
    "# Combine gainers and losers\n",
    "top_losers_gainers = pd.concat([top5, bottom5]).reset_index(drop=True)\n",
    "\n",
    "# Show the result\n",
    "print(top_losers_gainers[['Month', 'Ticker', 'return', 'Type']])\n",
    "\n",
    "# Optional: Save to CSV\n",
    "top_losers_gainers.to_csv(\"monthly_top_5_performance.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75980751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c438fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Establish the connection\n",
    "connection = pymysql.connect(\n",
    "        host=\"localhost\",\n",
    "        port=3306,\n",
    "        user=\"root\",\n",
    "        password=\"Rajarahi@22\",\n",
    "        database=\"newdatabase\"\n",
    "    )\n",
    "\n",
    "\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b189093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inserted data from 'volatility_analysis.csv' into 'volatility_analysis'\n",
      "‚úÖ Inserted data from 'cumulative_return_analysis.csv' into 'cumulative_return'\n",
      "‚úÖ Inserted data from 'sector_wise_performance.csv' into 'sector_performance'\n",
      "‚úÖ Inserted data from 'stock_price_correlation_matrix.csv' into 'stock_correlation'\n",
      "‚úÖ Inserted data from 'monthly_top_5_performance.csv' into 'monthly_top_5'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Dictionary mapping CSV filenames to table names\n",
    "csv_files = {\n",
    "    \"volatility_analysis.csv\": \"volatility_analysis\",\n",
    "    \"cumulative_return_analysis.csv\": \"cumulative_return\",\n",
    "    \"sector_wise_performance.csv\": \"sector_performance\",\n",
    "    \"stock_price_correlation_matrix.csv\": \"stock_correlation\",\n",
    "    \"monthly_top_5_performance.csv\": \"monthly_top_5\"\n",
    "}\n",
    "\n",
    "# Loop through each CSV file and insert its data into the corresponding SQL table\n",
    "for file_name, table_name in csv_files.items():\n",
    "    df = pd.read_csv(file_name)\n",
    "\n",
    "     # Replace NaN values with None to avoid SQL insertion errors\n",
    "    df = df.where(pd.notnull(df), None)\n",
    "    \n",
    "    # Create the table if it doesn't exist\n",
    "    cursor.execute(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "              {', '.join([f'`{col}` VARCHAR(255)' for col in df.columns])}\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    # Insert each row into the table\n",
    "    for index, row in df.iterrows():\n",
    "        values = [None if pd.isna(val) else str(val) for val in row]\n",
    "    try:\n",
    "        cursor.execute(\n",
    "            f\"INSERT INTO `{table_name}` ({', '.join([f'`{col}`' for col in df.columns])}) VALUES ({', '.join(['%s'] * len(values))})\", #f\"INSERT INTO {table_name} ({', '.join(df.columns)}) VALUES ({', '.join(['%s']*len(row))})\",\n",
    "            tuple(values)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inserting row {index}: {e}\")\n",
    "    connection.commit()\n",
    "    print(f\"‚úÖ Inserted data from '{file_name}' into '{table_name}'\")\n",
    "\n",
    "# Close the connection\n",
    "cursor.close()\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18ac23e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f1ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
